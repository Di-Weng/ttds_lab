{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from stemming.porter2 import stem\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_path = 'englishST.txt'\n",
    "def load_stopword(stopword_path):\n",
    "    stopword_list = []\n",
    "    with open(stopword_path, 'r',encoding='utf-8') as f1:\n",
    "        stopword_list = [str(current_word).strip() for current_word in f1]\n",
    "    return stopword_list\n",
    "\n",
    "stopword_list = load_stopword(stopword_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation_text(text):\n",
    "    \n",
    "    # simply remove every non-letter character\n",
    "    #     del_punc = r'[^A-Za-z0-9_-]' # keep _ -\n",
    "    #     del_punc = r'[^A-Za-z0-9_-\\']' #keep _ - '\n",
    "    del_punc = r'[\\W]' # keep _\n",
    "\n",
    "    text_nopunc = re.sub(del_punc,' ',text)\n",
    "    tokenisation_list = text_nopunc.split()\n",
    "    return tokenisation_list\n",
    "\n",
    "def lower_word(word_list):\n",
    "    return [current_word.strip().lower() for current_word in word_list]\n",
    "\n",
    "def token_lower_nostop_stem_list(all_text, stopword_list):\n",
    "    token_list = tokenisation_text(all_text)\n",
    "    token_lowerlist = lower_word(token_list)\n",
    "    token_lowerlist_nostop = [str(current_word) for current_word in token_lowerlist if str(current_word) not in stopword_list]\n",
    "    stem_list = [stem(current_word) for current_word in token_lowerlist_nostop]\n",
    "\n",
    "    return stem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_standard_xml(xml_file_path):\n",
    "    new_file_name = 'standard_' + xml_file_path.split('/')[-1]\n",
    "    new_file_path = data_folder + new_file_name\n",
    "\n",
    "    with open(new_file_path,'w',encoding='utf-8') as new_file:\n",
    "        with open(xml_file_path,'r') as original_file:\n",
    "            new_file.write('<?xml version=\"1.0\"?>\\n')\n",
    "            new_file.write('<sample>\\n')\n",
    "            for line in original_file:\n",
    "                if(len(line.strip()) == 0):\n",
    "                    continue\n",
    "                new_file.write(line)\n",
    "            new_file.write('</sample>')\n",
    "    return new_file_path\n",
    "\n",
    "def xml_sample(xml_file_path):\n",
    "    tree = ET.parse(trec_samle_path)\n",
    "    root = tree.getroot()\n",
    "    return root\n",
    "\n",
    "def merge_text(text1,text2):\n",
    "    merged_text = text1 + text2\n",
    "    return merged_text\n",
    "\n",
    "def xml_all_text(xml_file_path,stopword_list,is_sample):\n",
    "    if (is_sample):\n",
    "        # sample\n",
    "        standard_xml_path = to_standard_xml(sample_xml_path)\n",
    "        tree = ET.parse(standard_xml_path)\n",
    "        root = tree.getroot()\n",
    "        all_text_list = [(child.find('DOCNO').text,token_lower_nostop_stem_list(child.find('Text').text, stopword_list)) for child in\n",
    "                         root.findall(\"./DOC\")]\n",
    "    else:\n",
    "        # trec\n",
    "        standard_xml_path = to_standard_xml(trec_samle_path)\n",
    "        tree = ET.parse(standard_xml_path)\n",
    "        root = tree.getroot()\n",
    "        all_text_list = [(child.find('DOCNO').text, token_lower_nostop_stem_list(merge_text(child.find('HEADLINE').text,child.find('TEXT').text), stopword_list)) for child in root.findall(\"./DOC\")]\n",
    "\n",
    "    return all_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sample = 0\n",
    "data_folder = 'collections/'\n",
    "sample_txt_path = 'collections/sample.txt'\n",
    "trec_samle_path = 'collections/trec.sample.xml'\n",
    "sample_xml_path = 'collections/sample.xml'\n",
    "stopword_filepath = 'englishST.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_text_list= [doc1_stemlist, doc2_stemlist, .......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(is_sample):\n",
    "    all_text_list = xml_all_text(sample_xml_path,stopword_list,is_sample)\n",
    "else:\n",
    "    all_text_list = xml_all_text(trec_samle_path,stopword_list,is_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(all_text_list):\n",
    "    '''\n",
    "    return: # index dic[stem]={'doc_id':position}\n",
    "    '''\n",
    "\n",
    "    #get stem\n",
    "    stem_set = set()\n",
    "    for current_doc_tupple in all_text_list:\n",
    "        for current_stem in current_doc_tupple[1]:\n",
    "            stem_set.add(current_stem)\n",
    "\n",
    "    index_dic = {}\n",
    "    for current_stem in stem_set:\n",
    "        current_stem_doc_position_dic = {}\n",
    "        for current_doc_tupple in all_text_list:\n",
    "            current_doc_id = current_doc_tupple[0]\n",
    "            current_doc_text_list = current_doc_tupple[1]\n",
    "\n",
    "            if (current_stem not in current_doc_text_list):\n",
    "                continue\n",
    "\n",
    "            position_list = [index+1 for index in range(len(current_doc_text_list))\n",
    "                             if current_doc_text_list[index] == current_stem]\n",
    "            current_stem_doc_position_dic[str(current_doc_id)] = position_list\n",
    "        index_dic[current_stem] = current_stem_doc_position_dic\n",
    "    return index_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dic = inverted_index(all_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_index(index_dic,file_path):\n",
    "    sorted_index_dic_list = sorted(index_dic.items(),key=lambda x:x[0],reverse=False)\n",
    "    with open(file_path,'w',encoding='utf-8') as file_1:\n",
    "        for current_tupple in sorted_index_dic_list:\n",
    "            current_stem = current_tupple[0]\n",
    "            current_stem_doc_position_dic = current_tupple[1]\n",
    "            file_1.write(current_stem)\n",
    "            file_1.write(':\\n')\n",
    "            for current_doc_id,current_doc_id_position_list in current_stem_doc_position_dic.items():\n",
    "                file_1.write('\\t')\n",
    "                file_1.write(str(current_doc_id))\n",
    "                file_1.write(': ')\n",
    "                current_doc_id_position_list_new = map(lambda x: str(x), current_doc_id_position_list)\n",
    "                # print(','.join(current_doc_id_position_list_new))\n",
    "                file_1.write(','.join(current_doc_id_position_list_new))\n",
    "                file_1.write('\\n')\n",
    "            file_1.write('\\n')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output inverted dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_index_path = 'index.txt'\n",
    "output_index(index_dic,output_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load inverted index from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inverted_index(index_path):\n",
    "    result_inverted_index = {}\n",
    "    with open(index_path,'r') as f1:\n",
    "        for line in f1:\n",
    "            line = line.strip()\n",
    "            if(line.endswith(':')):\n",
    "                current_stem = line.replace(':','')\n",
    "                result_inverted_index[current_stem] = {}\n",
    "                continue\n",
    "                \n",
    "            if(len(line)==0):\n",
    "                continue\n",
    "                \n",
    "            temp_split_list = line.split(': ')\n",
    "            current_doc_id,str_position_list = temp_split_list[0],temp_split_list[1]\n",
    "            current_position_list = [int(current_position) for current_position in str_position_list.split(',')]\n",
    "            result_inverted_index[current_stem][current_doc_id] = current_position_list\n",
    "    return result_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_path = 'index.txt'\n",
    "loaded_inverted_index = load_inverted_index(inverted_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_id_set(current_inverted_dic):\n",
    "    doc_id_set = set()\n",
    "    for current_stem,current_doc_position_dic in current_inverted_dic.items():\n",
    "        for current_doc_id in current_doc_position_dic.keys():\n",
    "            doc_id_set.add(str(current_doc_id))\n",
    "    return doc_id_set\n",
    "\n",
    "def query_word(current_inverted_dic,current_word,is_not=0):\n",
    "    current_word_stem = stem(current_word.lower())\n",
    "    if(is_not):\n",
    "        for current_index_stem,current_index_stem_position in current_inverted_dic.items():\n",
    "            if(current_word_stem == current_index_stem):\n",
    "                doc_id_set = get_doc_id_set(current_inverted_dic)\n",
    "                stem_doc_list = set(current_index_stem_position.keys())\n",
    "                return list(doc_id_set.difference(stem_doc_list))\n",
    "    else:\n",
    "        for current_index_stem,current_index_stem_position in current_inverted_dic.items():\n",
    "            if(current_word_stem == current_index_stem):\n",
    "                return list(current_index_stem_position.keys())\n",
    "    raise RuntimeError('Query not found.')\n",
    "    return\n",
    "\n",
    "# OR\n",
    "def union_list(a,b):\n",
    "    result_list = list(set(a).union(set(b)))\n",
    "    result_list.sort(key=lambda i:int(i))\n",
    "    return result_list\n",
    "\n",
    "# AND\n",
    "def intersection_list(a,b):\n",
    "    result_list = list(set(a).intersection(set(b)))\n",
    "    result_list.sort(key=lambda i:int(i))\n",
    "    return result_list\n",
    "\n",
    "def probability_query(doc_word_pos1,doc_word_pos2,current_distance):\n",
    "    result_list = []\n",
    "    for current_docid_1,current_positionlist_1 in doc_word_pos1.items():\n",
    "        if(current_docid_1 not in doc_word_pos2.keys()):\n",
    "            continue\n",
    "        current_positionlist_2 = doc_word_pos2[current_docid_1]\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        while((i <= len(current_positionlist_1) - 1) and (j <= len(current_positionlist_2) - 1)):\n",
    "            if(int(current_positionlist_1[i]) > int(current_positionlist_2[j]) + current_distance):\n",
    "                j+=1\n",
    "                continue\n",
    "            elif(int(current_positionlist_1[i]) < int(current_positionlist_2[j]) - current_distance):\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                result_list.append(current_docid_1)\n",
    "                break\n",
    "    return result_list\n",
    "\n",
    "#phrase\n",
    "def phrase_query(current_inverted_dic,query_phrase):\n",
    "    phrase_list = [ stem(current_word.strip()) for current_word in query_phrase.replace('\"','').split()]\n",
    "    result_list = probability_query(current_inverted_dic[phrase_list[0]],current_inverted_dic[phrase_list[1]],1)\n",
    "    return result_list\n",
    "\n",
    "# word position query\n",
    "def word_position_query(current_inverted_dic, current_word):\n",
    "    '''\n",
    "    return: {'doc_id':[position_list]}\n",
    "    '''\n",
    "    if(current_word in current_inverted_dic.keys()):\n",
    "        return current_inverted_dic[current_word]\n",
    "    \n",
    "    raise RuntimeError('Query not found.')\n",
    "    return\n",
    "\n",
    "def boolean_query(current_inverted_dic, current_query_word):\n",
    "    is_NOT = 0\n",
    "    \n",
    "    #whether contains NOT\n",
    "    if(current_query_word.startswith('NOT')):\n",
    "        is_NOT = 1\n",
    "        current_query_word = current_query_word.replace('NOT'.'').strip()\n",
    "        \n",
    "    if(current_query_word.startswith('\"') and current_query_word.endswith('\"')):\n",
    "        result_list = phrase_query(loaded_index_dic,current_query_word)\n",
    "    elif(current_query_word.startswith('#')):\n",
    "        current_query_word_list = current_query_word.split('(')\n",
    "        current_word_distance = int(current_query_word_list[0].replace('#','').strip())\n",
    "        current_query_stem_list = [stem(temp_word.strip()) for temp_word in current_query_word_list[1].replace(')','').split(',')]\n",
    "        result_list = probability_query(loaded_index_dic[current_query_stem_list[0]],loaded_index_dic[current_query_stem_list[1]],current_word_distance)\n",
    "    else:\n",
    "        result = query_word(loaded_index_dic,current_query_word)\n",
    "    if(is_NOT):\n",
    "        doc_id_set = get_doc_id_set(current_inverted_dic)\n",
    "        result_list = list(doc_id_set.difference(set(result_list)))\n",
    "    \n",
    "    return result_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_path = 'queries.boolean.txt'\n",
    "operator_list = ['AND','OR']\n",
    "query_result_list = []\n",
    "if(query_file_path.split(.)[1] == 'boolean'):\n",
    "    with open(query_file_path,'r') as query_file:\n",
    "        for current_query_line in query_file:\n",
    "            current_query_line = current_query_line.strip()\n",
    "                if(len(current_querry_line) == 0):\n",
    "                    continue\n",
    "                current_query_id = current_query_line.split()[0] # get query id\n",
    "                current_query = ' '.join(current_query_line.split()[1:]) # get query text\n",
    "                \n",
    "                #check whether contains any operator: 'AND' or 'OR'\n",
    "                for current_operator in operator_list:\n",
    "                    result_list = []\n",
    "                    if(current_operator in current_query):\n",
    "                        current_query_split_list = current_query.split(current_operator)\n",
    "                        \n",
    "                        for current_query_word in current_operator:\n",
    "                            result_list.append(boolean_query(load_inverted_index,current_query_word))\n",
    "                        \n",
    "                        if(current_operator == 'AND'):\n",
    "                            \n",
    "                            \n",
    "                        \n",
    "                        break\n",
    "                \n",
    "        query_word_list =\n",
    "elif(query_file_path.split(.)[1] == 'ranked'):\n",
    "    \n",
    "    current_result = tf_idf_weight(loaded_index_dic,current_query,is_stop,current_stopword_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttds",
   "language": "python",
   "name": "ttds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
